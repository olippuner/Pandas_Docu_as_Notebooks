{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbda2c20-93b0-4540-bf8e-ca9b8c59c75f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72457261",
   "metadata": {},
   "source": [
    "\n",
    "<a id='scaletolarge-rst'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8adee1",
   "metadata": {},
   "source": [
    "# Scaling to large datasets\n",
    "\n",
    "pandas provides data structures for in-memory analytics, which makes using pandas\n",
    "to analyze datasets that are larger than memory datasets somewhat tricky. Even datasets\n",
    "that are a sizable fraction of memory become unwieldy, as some pandas operations need\n",
    "to make intermediate copies.\n",
    "\n",
    "This document provides a few recommendations for scaling your analysis to larger datasets.\n",
    "It’s a complement to enhancingperf, which focuses on speeding up analysis\n",
    "for datasets that fit in memory.\n",
    "\n",
    "But first, it’s worth considering *not using pandas*. pandas isn’t the right\n",
    "tool for all situations. If you’re working with very large datasets and a tool\n",
    "like PostgreSQL fits your needs, then you should probably be using that.\n",
    "Assuming you want or need the expressiveness and power of pandas, let’s carry on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b634f498",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pyarrow  \n",
    "# if not installed use: pip install pyarrow \n",
    "\n",
    "print(\"pandas\", pd.__version__)\n",
    "print(\"numpy\",np.__version__)\n",
    "print(\"pyarrow\",pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df17a18",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "from pandas._testing import _make_timeseries\n",
    "\n",
    "# Make a random in-memory dataset\n",
    "ts = _make_timeseries(freq=\"30S\", seed=0)\n",
    "ts.to_csv(\"timeseries.csv\")\n",
    "ts.to_parquet(\"timeseries.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a0098",
   "metadata": {},
   "source": [
    "## Load less data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b031c52",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "# make a similar dataset with many columns\n",
    "timeseries = [\n",
    "    _make_timeseries(freq=\"1T\", seed=i).rename(columns=lambda x: f\"{x}_{i}\")\n",
    "    for i in range(10)\n",
    "]\n",
    "ts_wide = pd.concat(timeseries, axis=1)\n",
    "ts_wide.to_parquet(\"timeseries_wide.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524b355f",
   "metadata": {},
   "source": [
    "Suppose our raw dataset on disk has many columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dd8359-7dad-49cd-9748-4980ede4d618",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "``` Python\n",
    "                     id_0    name_0       x_0       y_0  id_1   name_1       x_1  ...  name_8       x_8       y_8  id_9   name_9       x_9       y_9\n",
    "timestamp                                                                         ...\n",
    "2000-01-01 00:00:00  1015   Michael -0.399453  0.095427   994    Frank -0.176842  ...     Dan -0.315310  0.713892  1025   Victor -0.135779  0.346801\n",
    "2000-01-01 00:01:00   969  Patricia  0.650773 -0.874275  1003    Laura  0.459153  ...  Ursula  0.913244 -0.630308  1047    Wendy -0.886285  0.035852\n",
    "2000-01-01 00:02:00  1016    Victor -0.721465 -0.584710  1046  Michael  0.524994  ...     Ray -0.656593  0.692568  1064   Yvonne  0.070426  0.432047\n",
    "2000-01-01 00:03:00   939     Alice -0.746004 -0.908008   996   Ingrid -0.414523  ...   Jerry -0.958994  0.608210   978    Wendy  0.855949 -0.648988\n",
    "2000-01-01 00:04:00  1017       Dan  0.919451 -0.803504  1048    Jerry -0.569235  ...   Frank -0.577022 -0.409088   994      Bob -0.270132  0.335176\n",
    "...                   ...       ...       ...       ...   ...      ...       ...  ...     ...       ...       ...   ...      ...       ...       ...\n",
    "2000-12-30 23:56:00   999       Tim  0.162578  0.512817   973    Kevin -0.403352  ...     Tim -0.380415  0.008097  1041  Charlie  0.191477 -0.599519\n",
    "2000-12-30 23:57:00   970     Laura -0.433586 -0.600289   958   Oliver -0.966577  ...   Zelda  0.971274  0.402032  1038   Ursula  0.574016 -0.930992\n",
    "2000-12-30 23:58:00  1065     Edith  0.232211 -0.454540   971      Tim  0.158484  ...   Alice -0.222079 -0.919274  1022      Dan  0.031345 -0.657755\n",
    "2000-12-30 23:59:00  1019    Ingrid  0.322208 -0.615974   981   Hannah  0.607517  ...   Sarah -0.424440 -0.117274   990   George -0.375530  0.563312\n",
    "2000-12-31 00:00:00   937    Ursula -0.906523  0.943178  1018    Alice -0.564513  ...   Jerry  0.236837  0.807650   985   Oliver  0.777642  0.783392\n",
    "\n",
    "[525601 rows x 40 columns]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f7db3c",
   "metadata": {},
   "source": [
    "To load the columns we want, we have two options.\n",
    "Option 1 loads in all the data and then filters to what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19771ec8",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "columns = [\"id_0\", \"name_0\", \"x_0\", \"y_0\"]\n",
    "\n",
    "pd.read_parquet(\"timeseries_wide.parquet\")[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571afff2",
   "metadata": {},
   "source": [
    "Option 2 only loads the columns we request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24bc13a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "pd.read_parquet(\"timeseries_wide.parquet\", columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49106599",
   "metadata": {},
   "source": [
    "If we were to measure the memory usage of the two calls, we’d see that specifying\n",
    "`columns` uses about 1/10th the memory in this case.\n",
    "\n",
    "With `pandas.read_csv()`, you can specify `usecols` to limit the columns\n",
    "read into memory. Not all file formats that can be read by pandas provide an option\n",
    "to read a subset of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e548e88",
   "metadata": {},
   "source": [
    "## Use efficient datatypes\n",
    "\n",
    "The default pandas data types are not the most memory efficient. This is\n",
    "especially true for text data columns with relatively few unique values (commonly\n",
    "referred to as “low-cardinality” data). By using more efficient data types, you\n",
    "can store larger datasets in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ed561",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ts = pd.read_parquet(\"timeseries.parquet\")\n",
    "ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c748a0",
   "metadata": {},
   "source": [
    "Now, let’s inspect the data types and memory usage to see where we should focus our\n",
    "attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb571d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ts.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9324eea",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ts.memory_usage(deep=True)  # memory usage in bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdf54b",
   "metadata": {},
   "source": [
    "The `name` column is taking up much more memory than any other. It has just a\n",
    "few unique values, so it’s a good candidate for converting to a\n",
    "`Categorical`. With a Categorical, we store each unique name once and use\n",
    "space-efficient integers to know which specific name is used in each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d2da2",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ts2 = ts.copy()\n",
    "ts2[\"name\"] = ts2[\"name\"].astype(\"category\")\n",
    "ts2.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab63ce8",
   "metadata": {},
   "source": [
    "We can go a bit further and downcast the numeric columns to their smallest types\n",
    "using `pandas.to_numeric()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54d3226",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ts2[\"id\"] = pd.to_numeric(ts2[\"id\"], downcast=\"unsigned\")\n",
    "ts2[[\"x\", \"y\"]] = ts2[[\"x\", \"y\"]].apply(pd.to_numeric, downcast=\"float\")\n",
    "ts2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2f20e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ts2.memory_usage(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdeeb4",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "reduction = ts2.memory_usage(deep=True).sum() / ts.memory_usage(deep=True).sum()\n",
    "print(f\"{reduction:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527a105",
   "metadata": {},
   "source": [
    "In all, we’ve reduced the in-memory footprint of this dataset to 1/5 of its\n",
    "original size.\n",
    "\n",
    "See categorical for more on `Categorical` and basics.dtypes\n",
    "for an overview of all of pandas’ dtypes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c716b",
   "metadata": {},
   "source": [
    "## Use chunking\n",
    "\n",
    "Some workloads can be achieved with chunking: splitting a large problem like “convert this\n",
    "directory of CSVs to parquet” into a bunch of small problems (“convert this individual CSV\n",
    "file into a Parquet file. Now repeat that for each file in this directory.”). As long as each chunk\n",
    "fits in memory, you can work with datasets that are much larger than memory.\n",
    "\n",
    ">**Note**\n",
    ">\n",
    ">Chunking works well when the operation you’re performing requires zero or minimal\n",
    "coordination between chunks. For more complicated workflows, you’re better off\n",
    "[using another library](#scale-other-libraries).\n",
    "\n",
    "Suppose we have an even larger “logical dataset” on disk that’s a directory of parquet\n",
    "files. Each file in the directory represents a different year of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca61326d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "N = 12\n",
    "starts = [f\"20{i:>02d}-01-01\" for i in range(N)]\n",
    "ends = [f\"20{i:>02d}-12-13\" for i in range(N)]\n",
    "\n",
    "pathlib.Path(\"../data/timeseries\").mkdir(exist_ok=True)\n",
    "\n",
    "for i, (start, end) in enumerate(zip(starts, ends)):\n",
    "    ts = _make_timeseries(start=start, end=end, freq=\"1T\", seed=i)\n",
    "    ts.to_parquet(f\"../data/timeseries/ts-{i:0>2d}.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506527f",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "data\n",
    "└── timeseries\n",
    "    ├── ts-00.parquet\n",
    "    ├── ts-01.parquet\n",
    "    ├── ts-02.parquet\n",
    "    ├── ts-03.parquet\n",
    "    ├── ts-04.parquet\n",
    "    ├── ts-05.parquet\n",
    "    ├── ts-06.parquet\n",
    "    ├── ts-07.parquet\n",
    "    ├── ts-08.parquet\n",
    "    ├── ts-09.parquet\n",
    "    ├── ts-10.parquet\n",
    "    └── ts-11.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5003f",
   "metadata": {},
   "source": [
    "Now we’ll implement an out-of-core `value_counts`. The peak memory usage of this\n",
    "workflow is the single largest chunk, plus a small series storing the unique value\n",
    "counts up to this point. As long as each individual file fits in memory, this will\n",
    "work for arbitrary-sized datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbeb6e",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "files = pathlib.Path(\"../data/timeseries/\").glob(\"ts*.parquet\")\n",
    "counts = pd.Series(dtype=int)\n",
    "for path in files:\n",
    "    # Only one dataframe is in memory at a time...\n",
    "    df = pd.read_parquet(path)\n",
    "    # ... plus a small Series ``counts``, which is updated.\n",
    "    counts = counts.add(df[\"name\"].value_counts(), fill_value=0)\n",
    "counts.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b712d9c",
   "metadata": {},
   "source": [
    "Some readers, like `pandas.read_csv()`, offer parameters to control the\n",
    "`chunksize` when reading a single file.\n",
    "\n",
    "Manually chunking is an OK option for workflows that don’t\n",
    "require too sophisticated of operations. Some operations, like `groupby`, are\n",
    "much harder to do chunkwise. In these cases, you may be better switching to a\n",
    "different library that implements these out-of-core algorithms for you.\n",
    "\n",
    "\n",
    "<a id='scale-other-libraries'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782514a",
   "metadata": {},
   "source": [
    "## Use other libraries\n",
    "\n",
    "pandas is just one library offering a DataFrame API. Because of its popularity,\n",
    "pandas’ API has become something of a standard that other libraries implement.\n",
    "The pandas documentation maintains a list of libraries implementing a DataFrame API\n",
    "in [our ecosystem page](../ecosystem.ipynb#ecosystem-out-of-core).\n",
    "\n",
    "For example, [Dask](https://dask.org), a parallel computing library, has [dask.dataframe](https://docs.dask.org/en/latest/dataframe.html), a\n",
    "pandas-like API for working with larger than memory datasets in parallel. Dask\n",
    "can use multiple threads or processes on a single machine, or a cluster of\n",
    "machines to process data in parallel.\n",
    "\n",
    "We’ll import `dask.dataframe` and notice that the API feels similar to pandas.\n",
    "We can use Dask’s `read_parquet` function, but provide a globstring of files to read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06a269",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.read_parquet(\"../data/timeseries/ts*.parquet\", engine=\"pyarrow\")\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb336c4",
   "metadata": {},
   "source": [
    "Inspecting the `ddf` object, we see a few things\n",
    "\n",
    "- There are familiar attributes like `.columns` and `.dtypes`  \n",
    "- There are familiar methods like `.groupby`, `.sum`, etc.  \n",
    "- There are new attributes like `.npartitions` and `.divisions`  \n",
    "\n",
    "\n",
    "The partitions and divisions are how Dask parallelizes computation. A **Dask**\n",
    "DataFrame is made up of many pandas DataFrames. A single method call on a\n",
    "Dask DataFrame ends up making many pandas method calls, and Dask knows how to\n",
    "coordinate everything to get the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a30367-e08e-4b70-9ec1-e2d9a0f86f00",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe9e0b-7e84-4bfb-82dc-b437536b2b4c",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcf2fa1-5824-465e-ac63-951f286af812",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b564a76",
   "metadata": {},
   "source": [
    "One major difference: the `dask.dataframe` API is *lazy*. If you look at the\n",
    "repr above, you’ll notice that the values aren’t actually printed out; just the\n",
    "column names and dtypes. That’s because Dask hasn’t actually read the data yet.\n",
    "Rather than executing immediately, doing operations build up a **task graph**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb8e9ce-a0da-413b-a0b0-cdfad9326722",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a74610-1ee5-484a-852f-e1e7be1360fc",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1daacc75-a9a1-4020-a114-812ad5bec320",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf[\"name\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bb8e6c",
   "metadata": {},
   "source": [
    "Each of these calls is instant because the result isn’t being computed yet.\n",
    "We’re just building up a list of computation to do when someone needs the\n",
    "result. Dask knows that the return type of a `pandas.Series.value_counts`\n",
    "is a pandas Series with a certain dtype and a certain name. So the Dask version\n",
    "returns a Dask Series with the same dtype and the same name.\n",
    "\n",
    "To get the actual result you can call `.compute()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c73180",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%time ddf[\"name\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24226bef",
   "metadata": {},
   "source": [
    "At that point, you get back the same thing you’d get with pandas, in this case\n",
    "a concrete pandas Series with the count of each `name`.\n",
    "\n",
    "Calling `.compute` causes the full task graph to be executed. This includes\n",
    "reading the data, selecting the columns, and doing the `value_counts`. The\n",
    "execution is done *in parallel* where possible, and Dask tries to keep the\n",
    "overall memory footprint small. You can work with datasets that are much larger\n",
    "than memory, as long as each partition (a regular pandas DataFrame) fits in memory.\n",
    "\n",
    "By default, `dask.dataframe` operations use a threadpool to do operations in\n",
    "parallel. We can also connect to a cluster to distribute the work on many\n",
    "machines. In this case we’ll connect to a local “cluster” made up of several\n",
    "processes on this single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ff8ed",
   "metadata": {
    "hide-output": false
   },
   "source": [
    "```python\n",
    ">>> from dask.distributed import Client, LocalCluster\n",
    "\n",
    ">>> cluster = LocalCluster()\n",
    ">>> client = Client(cluster)\n",
    ">>> client\n",
    "<Client: 'tcp://127.0.0.1:53349' processes=4 threads=8, memory=17.18 GB>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47cdfce",
   "metadata": {},
   "source": [
    "Once this `client` is created, all of Dask’s computation will take place on\n",
    "the cluster (which is just processes in this case).\n",
    "\n",
    "Dask implements the most used parts of the pandas API. For example, we can do\n",
    "a familiar groupby aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1942e3d",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "%time ddf.groupby(\"name\")[[\"x\", \"y\"]].mean().compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e788f7e0",
   "metadata": {},
   "source": [
    "The grouping and aggregation is done out-of-core and in parallel.\n",
    "\n",
    "When Dask knows the `divisions` of a dataset, certain optimizations are\n",
    "possible. When reading parquet datasets written by dask, the divisions will be\n",
    "known automatically. In this case, since we created the parquet files manually,\n",
    "we need to supply the divisions manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fa54a",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "N = 12\n",
    "starts = [f\"20{i:>02d}-01-01\" for i in range(N)]\n",
    "ends = [f\"20{i:>02d}-12-13\" for i in range(N)]\n",
    "\n",
    "divisions = tuple(pd.to_datetime(starts)) + (pd.Timestamp(ends[-1]),)\n",
    "ddf.divisions = divisions\n",
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6e77c1",
   "metadata": {},
   "source": [
    "Now we can do things like fast random access with `.loc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bd201",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf.loc[\"2002-01-01 12:01\":\"2002-01-01 12:05\"].compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942769a",
   "metadata": {},
   "source": [
    "Dask knows to just look in the 3rd partition for selecting values in 2002. It\n",
    "doesn’t need to look at any other data.\n",
    "\n",
    "Many workflows involve a large amount of data and processing it in a way that\n",
    "reduces the size to something that fits in memory. In this case, we’ll resample\n",
    "to daily frequency and take the mean. Once we’ve taken the mean, we know the\n",
    "results will fit in memory, so we can safely call `compute` without running\n",
    "out of memory. At that point it’s just a regular pandas object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b2fafd",
   "metadata": {
    "hide-output": false
   },
   "outputs": [],
   "source": [
    "ddf[[\"x\", \"y\"]].resample(\"1D\").mean().cumsum().compute().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69932b97",
   "metadata": {},
   "source": [
    "These Dask examples have all be done using multiple processes on a single\n",
    "machine. Dask can be [deployed on a cluster](https://docs.dask.org/en/latest/setup.html) to scale up to even larger\n",
    "datasets.\n",
    "\n",
    "You see more dask examples at [https://examples.dask.org](https://examples.dask.org)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3f56d9-fd84-4402-a59c-60dc0dc3809e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "date": 1630231092.255614,
  "filename": "42_scaleToLarge.rst",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "title": "Scaling to large datasets"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
